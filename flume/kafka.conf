# flume conf file

kagent.sources = source1
kagent.sinks = s3
kagent.channels = channel1

kagent.sources.source1.type = com.netuitive.AnalyticsSplitSource
kagent.sources.source1.channels = channel1
kagent.sources.source1.batchSize = 100
kagent.sources.source1.batchDurationMillis = 2000
kagent.sources.source1.zookeeperConnect = zookeeper
kagent.sources.source1.topic = analysis.results
kagent.sources.source1.groupId = flume.dpl.analysis.results

# Describe the sink
kagent.sinks.sink1.type = logger

# SINK CSV
kagent.sinks.s3.type = hdfs
kagent.sinks.s3.hdfs.path = s3n://username:password@bucket/%Y/%m/%d/%H
kagent.sinks.s3.hdfs.fileType = DataStream
kagent.sinks.s3.hdfs.filePrefix = flume-s3
kagent.sinks.s3.hdfs.fileSuffix = .csv
kagent.sinks.s3.hdfs.writeFormat = Text
kagent.sinks.s3.hdfs.callTimeout = 60000
kagent.sinks.s3.hdfs.threadsPoolSize = 100
kagent.sinks.s3.hdfs.rollTimerPoolSize = 10

### 2^8 lines
kagent.sinks.s3.hdfs.rollCount = 65536

### 8MB
kagent.sinks.s3.hdfs.rollSize = 536870912

### 5 Minutes
kagent.sinks.s3.hdfs.rollInterval = 300

# AVRO PIP
kagent.sinks.s3.serializer=com.netuitive.CsvSerializer$Builder

# Use a channel which buffers events in memory
kagent.channels.channel1.type = memory
kagent.channels.channel1.capacity = 100000
kagent.channels.channel1.transactionCapacity = 10000

# Bind the source and sink to the channel
kagent.sources.source1.channels = channel1
kagent.sinks.s3.channel = channel1

